{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough project/code architecture\n",
    "\n",
    "1. Load MNIST into some kind of native dataformat (i.e. not with a oneliner that gives you a pytorch dataset)\n",
    "1. Load entire MNIST into a dataset, then make that into a dataloader\n",
    "1. Train MNIST with pytorch.  make a note of loss and accuracy. You’ll need to choose an architecture, batch size and number of epochs to train for. \n",
    "    * Get architecture from pytorch docs. Should be simple. \n",
    "    * Number of epochs is “until results get as good as they are going to get”, i.e. by inspection.\n",
    "    * Batch size I got no idea. 1 is probably fine, but feel free to try others.\n",
    "1. Split MNIST into N groups randomly. Create N datasets. Train with federated.py. You should get roughly the final loss and accuracy you saw previously. \n",
    "    * Use the same architecture as before.\n",
    "    * Choose number of rounds same way you chose number of epochs in previous step, i.e. until it converges.\n",
    "    * Number of epochs per round. Maybe try 1, 5 and 10? Go with 1 unless you get very different results.\n",
    "    * Batch size should probably be the same as before.\n",
    "1. Repeat, but split MNIST into N groups with the deck stacked (possibly as extremely as 10 groups each containing data from only one class).\n",
    "\n",
    "### Snippets\n",
    "\n",
    "#### Visualize an array\n",
    "See cell 109 here: https://github.com/williamsmj/pytorch-notes/blob/master/pytorch-60-minute-blitz.ipynb\n",
    "\n",
    "#### Change native data to dataset, change dataset to native data, and loop over dataset doing something other than training\n",
    "See https://github.com/williamsmj/pytorch-notes/blob/master/dataset_manipulation.ipynb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './MNIST-data/raw'\n",
    "\n",
    "# location of data and labels\n",
    "test_labels_file = data_path + '/' + 't10k-labels-idx1-ubyte'\n",
    "test_data_file = data_path + '/' + 't10k-images-idx3-ubyte'\n",
    "train_labels_file = data_path + '/' + 'train-labels-idx1-ubyte'\n",
    "train_data_file = data_path + '/' + 'train-images-idx3-ubyte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# pytorch datasets that download MNIST set as needed; used only to download files\n",
    "train_dset = dsets.MNIST(root=data_path, download=False, train=True, transform=trans)\n",
    "test_dset = dsets.MNIST(root=data_path, download=False, train=False, transform=trans)\n",
    "\n",
    "#print(\"Training dset:\", train_dset)\n",
    "#print(\"Test dset:\", test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data into numpy arrays\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "    \n",
    "# read data and label files into numpy arrays\n",
    "test_data = read_idx(test_data_file)\n",
    "test_labels = read_idx(test_labels_file)\n",
    "train_data = read_idx(train_data_file)\n",
    "train_labels = read_idx(train_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnp.savetxt(\"./MNIST-data/raw/test_data.csv\", test_data.reshape(len(test_data), 784), delimiter=\\',\\')\\nnp.savetxt(\"./MNIST-data/raw/test_labels.csv\", test_labels, delimiter=\\',\\')\\nnp.savetxt(\"./MNIST-data/raw/train_data.csv\", train_data.reshape(len(train_data), 784), delimiter=\\',\\')\\nnp.savetxt(\"./MNIST-data/raw/train_labels.csv\", train_labels, delimiter=\\',\\')\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save CSV files if needed\n",
    "\n",
    "\"\"\"\n",
    "np.savetxt(\"./MNIST-data/raw/test_data.csv\", test_data.reshape(len(test_data), 784), delimiter=',')\n",
    "np.savetxt(\"./MNIST-data/raw/test_labels.csv\", test_labels, delimiter=',')\n",
    "np.savetxt(\"./MNIST-data/raw/train_data.csv\", train_data.reshape(len(train_data), 784), delimiter=',')\n",
    "np.savetxt(\"./MNIST-data/raw/train_labels.csv\", train_labels, delimiter=',')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# to restore\n",
    "#np.loadtext()\n",
    "#reshape((len(), 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# pick a random sample from the sets of observations\\nimport random\\ntest_sample_num = random.randint(0, len(test_labels))\\ntrain_sample_num = random.randint(0, len(train_labels))\\n\\n# print some arrays to confirm we have data\\nprint(\"Test labels:\", test_labels)\\nprint(\"Test data:\", test_data[test_sample_num])\\nprint(\"Training labels:\", train_labels)\\nprint(\"Training data:\", train_data[train_sample_num])\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the data in the arrays\n",
    "\n",
    "\"\"\"\n",
    "# pick a random sample from the sets of observations\n",
    "import random\n",
    "test_sample_num = random.randint(0, len(test_labels))\n",
    "train_sample_num = random.randint(0, len(train_labels))\n",
    "\n",
    "# print some arrays to confirm we have data\n",
    "print(\"Test labels:\", test_labels)\n",
    "print(\"Test data:\", test_data[test_sample_num])\n",
    "print(\"Training labels:\", train_labels)\n",
    "print(\"Training data:\", train_data[train_sample_num])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-64ab5bd020e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_sample_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_sample_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test sample index: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sample_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# visualize some test samples\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = test_data[test_sample_num]\n",
    "label = test_labels[test_sample_num]\n",
    "print('Test sample index: ', test_sample_num)\n",
    "print('Sample value: ', label)\n",
    "plt.imshow(img)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Test sample histogram:')\\nplt.hist(test_labels, bins=10)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how is the test data distributed?\n",
    "\n",
    "\"\"\"\n",
    "print('Test sample histogram:')\n",
    "plt.hist(test_labels, bins=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimg = train_data[train_sample_num]\\nlabel = train_labels[train_sample_num]\\nprint('Training sample index: ', train_sample_num)\\nprint('Sample value: ', label)\\nplt.imshow(img)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize a training sample\n",
    "\n",
    "\"\"\"\n",
    "img = train_data[train_sample_num]\n",
    "label = train_labels[train_sample_num]\n",
    "print('Training sample index: ', train_sample_num)\n",
    "print('Sample value: ', label)\n",
    "plt.imshow(img)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Training data histogram:')\\nplt.hist(train_labels, bins=10)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how is the training data distributed?\n",
    "\n",
    "\"\"\"\n",
    "print('Training data histogram:')\n",
    "plt.hist(train_labels, bins=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-34876cb58ef6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# load tensors into datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtest_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mtrain_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "# load numpy arrays into pytorch Datasets\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# create tensors from np arrays\n",
    "test_data_tensor = torch.from_numpy(test_data)\n",
    "test_labels_tensor = torch.from_numpy(test_labels)\n",
    "train_data_tensor = torch.from_numpy(train_data)\n",
    "train_labels_tensor = torch.from_numpy(train_labels)\n",
    "\n",
    "\n",
    "# TODO: TRANSFORM THE DATA WHEN THE SETS ARE CREATED\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "\"\"\"\n",
    "\n",
    "# load tensors into datasets\n",
    "test_dset = TensorDataset(test_data_tensor, test_labels_tensor, transform=trans)\n",
    "train_dset = TensorDataset(train_data_tensor, train_labels_tensor, transform=trans)\n",
    "\n",
    "print(\"test_dset:\",\n",
    "      test_dset,\n",
    "      len(test_dset), test_dset[0][0].size(), type(test_dset[0][0])\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dloader 10000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 1\n",
    "test_dloader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)\n",
    "train_dloader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"test_dloader\", len(test_dloader),\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up network\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "## network\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MLP\"\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"LeNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLPNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8c0791d71a1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0003\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLPNet' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "model = MLPNet()\n",
    "\n",
    "learning_rate = 0.0003\n",
    "momentum = 0.5\n",
    "num_epochs = 2\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#print(enumerate(train_dloader))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = 0\n",
    "\n",
    "    # train\n",
    "    for batch_idx, (x, target) in enumerate(train_dloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_dloader):\n",
    "        #    print(\"batch_idx:\", batch_idx)\n",
    "        #print(\"shape of x:\", x.shape, type(x))\n",
    "        #print(x[0])\n",
    "        #print(\"shape of target:\", target.shape, type(target))\n",
    "        \n",
    "        # TODO try without the next line\n",
    "        #x, target = Variable(x), Variable(target)\n",
    "        \n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        avg_loss = avg_loss * 0.99 + loss.item() * 0.01\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 500 == 0 or (batch_idx+1) == len(train_dloader):\n",
    "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, avg_loss))\n",
    "\n",
    "            \n",
    "            \n",
    "# ADD PLOTTING\n",
    "\n",
    "# ADD TESTING AFTER TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 100, train loss: 2.298744\n",
      "==>>> epoch: 0, batch index: 200, train loss: 1.527318\n",
      "==>>> epoch: 0, batch index: 300, train loss: 1.569507\n",
      "==>>> epoch: 0, batch index: 400, train loss: 1.890597\n",
      "==>>> epoch: 0, batch index: 500, train loss: 2.413005\n",
      "==>>> epoch: 0, batch index: 600, train loss: 1.716513\n",
      "==>>> epoch: 0, batch index: 700, train loss: 1.455534\n",
      "==>>> epoch: 0, batch index: 800, train loss: 2.105138\n",
      "==>>> epoch: 0, batch index: 900, train loss: 2.461570\n",
      "==>>> epoch: 0, batch index: 1000, train loss: 1.135260\n",
      "==>>> epoch: 0, batch index: 1100, train loss: 1.655213\n",
      "==>>> epoch: 0, batch index: 1200, train loss: 1.911636\n",
      "==>>> epoch: 0, batch index: 1300, train loss: 1.575496\n",
      "==>>> epoch: 0, batch index: 1400, train loss: 1.406858\n",
      "==>>> epoch: 0, batch index: 1500, train loss: 1.582742\n",
      "==>>> epoch: 0, batch index: 1600, train loss: 0.618945\n",
      "==>>> epoch: 0, batch index: 1700, train loss: 1.205498\n",
      "==>>> epoch: 0, batch index: 1800, train loss: 2.589466\n",
      "==>>> epoch: 0, batch index: 1900, train loss: 2.500826\n",
      "==>>> epoch: 0, batch index: 2000, train loss: 2.284937\n",
      "==>>> epoch: 0, batch index: 2100, train loss: 2.268441\n",
      "==>>> epoch: 0, batch index: 2200, train loss: 2.329942\n",
      "==>>> epoch: 0, batch index: 2300, train loss: 2.392937\n",
      "==>>> epoch: 0, batch index: 2400, train loss: 2.326686\n",
      "==>>> epoch: 0, batch index: 2500, train loss: 2.341557\n",
      "==>>> epoch: 0, batch index: 2600, train loss: 2.343361\n",
      "==>>> epoch: 0, batch index: 2700, train loss: 2.321528\n",
      "==>>> epoch: 0, batch index: 2800, train loss: 2.275344\n",
      "==>>> epoch: 0, batch index: 2900, train loss: 2.340427\n",
      "==>>> epoch: 0, batch index: 3000, train loss: 2.298070\n",
      "==>>> epoch: 0, batch index: 3100, train loss: 2.319495\n",
      "==>>> epoch: 0, batch index: 3200, train loss: 2.322703\n",
      "==>>> epoch: 0, batch index: 3300, train loss: 2.354826\n",
      "==>>> epoch: 0, batch index: 3400, train loss: 2.321386\n",
      "==>>> epoch: 0, batch index: 3500, train loss: 2.360072\n",
      "==>>> epoch: 0, batch index: 3600, train loss: 2.290405\n",
      "==>>> epoch: 0, batch index: 3700, train loss: 2.293056\n",
      "==>>> epoch: 0, batch index: 3800, train loss: 2.295907\n",
      "==>>> epoch: 0, batch index: 3900, train loss: 2.354322\n",
      "==>>> epoch: 0, batch index: 4000, train loss: 2.436957\n",
      "==>>> epoch: 0, batch index: 4100, train loss: 2.328614\n",
      "==>>> epoch: 0, batch index: 4200, train loss: 2.255129\n",
      "==>>> epoch: 0, batch index: 4300, train loss: 2.378400\n",
      "==>>> epoch: 0, batch index: 4400, train loss: 2.242042\n",
      "==>>> epoch: 0, batch index: 4500, train loss: 2.401563\n",
      "==>>> epoch: 0, batch index: 4600, train loss: 2.312163\n",
      "==>>> epoch: 0, batch index: 4700, train loss: 2.270425\n",
      "==>>> epoch: 0, batch index: 4800, train loss: 2.361783\n",
      "==>>> epoch: 0, batch index: 4900, train loss: 2.329070\n",
      "==>>> epoch: 0, batch index: 5000, train loss: 2.353033\n",
      "==>>> epoch: 0, batch index: 5100, train loss: 2.353435\n",
      "==>>> epoch: 0, batch index: 5200, train loss: 2.349618\n",
      "==>>> epoch: 0, batch index: 5300, train loss: 2.317060\n",
      "==>>> epoch: 0, batch index: 5400, train loss: 2.302018\n",
      "==>>> epoch: 0, batch index: 5500, train loss: 2.274449\n",
      "==>>> epoch: 0, batch index: 5600, train loss: 2.305876\n",
      "==>>> epoch: 0, batch index: 5700, train loss: 2.402821\n",
      "==>>> epoch: 0, batch index: 5800, train loss: 2.340795\n",
      "==>>> epoch: 0, batch index: 5900, train loss: 2.303288\n",
      "==>>> epoch: 0, batch index: 6000, train loss: 2.300367\n",
      "==>>> epoch: 0, batch index: 6100, train loss: 2.329418\n",
      "==>>> epoch: 0, batch index: 6200, train loss: 2.322214\n",
      "==>>> epoch: 0, batch index: 6300, train loss: 2.395916\n",
      "==>>> epoch: 0, batch index: 6400, train loss: 2.373939\n",
      "==>>> epoch: 0, batch index: 6500, train loss: 2.338223\n",
      "==>>> epoch: 0, batch index: 6600, train loss: 2.313386\n",
      "==>>> epoch: 0, batch index: 6700, train loss: 2.435164\n",
      "==>>> epoch: 0, batch index: 6800, train loss: 2.343011\n",
      "==>>> epoch: 0, batch index: 6900, train loss: 2.339704\n",
      "==>>> epoch: 0, batch index: 7000, train loss: 2.315944\n",
      "==>>> epoch: 0, batch index: 7100, train loss: 2.385464\n",
      "==>>> epoch: 0, batch index: 7200, train loss: 2.383123\n",
      "==>>> epoch: 0, batch index: 7300, train loss: 2.213475\n",
      "==>>> epoch: 0, batch index: 7400, train loss: 2.319152\n",
      "==>>> epoch: 0, batch index: 7500, train loss: 2.365728\n",
      "==>>> epoch: 0, batch index: 7600, train loss: 2.350171\n",
      "==>>> epoch: 0, batch index: 7700, train loss: 2.403025\n",
      "==>>> epoch: 0, batch index: 7800, train loss: 2.354322\n",
      "==>>> epoch: 0, batch index: 7900, train loss: 2.271849\n",
      "==>>> epoch: 0, batch index: 8000, train loss: 2.297466\n",
      "==>>> epoch: 0, batch index: 8100, train loss: 2.353693\n",
      "==>>> epoch: 0, batch index: 8200, train loss: 2.313207\n",
      "==>>> epoch: 0, batch index: 8300, train loss: 2.301414\n",
      "==>>> epoch: 0, batch index: 8400, train loss: 2.303736\n",
      "==>>> epoch: 0, batch index: 8500, train loss: 2.334461\n",
      "==>>> epoch: 0, batch index: 8600, train loss: 2.360157\n",
      "==>>> epoch: 0, batch index: 8700, train loss: 2.339583\n",
      "==>>> epoch: 0, batch index: 8800, train loss: 2.285791\n",
      "==>>> epoch: 0, batch index: 8900, train loss: 2.315783\n",
      "==>>> epoch: 0, batch index: 9000, train loss: 2.389649\n",
      "==>>> epoch: 0, batch index: 9100, train loss: 2.330781\n",
      "==>>> epoch: 0, batch index: 9200, train loss: 2.325563\n",
      "==>>> epoch: 0, batch index: 9300, train loss: 2.371655\n",
      "==>>> epoch: 0, batch index: 9400, train loss: 2.317656\n",
      "==>>> epoch: 0, batch index: 9500, train loss: 2.324032\n",
      "==>>> epoch: 0, batch index: 9600, train loss: 2.282062\n",
      "==>>> epoch: 0, batch index: 9700, train loss: 2.400622\n",
      "==>>> epoch: 0, batch index: 9800, train loss: 2.315785\n",
      "==>>> epoch: 0, batch index: 9900, train loss: 2.195094\n",
      "==>>> epoch: 0, batch index: 10000, train loss: 2.313865\n",
      "==>>> epoch: 0, batch index: 10100, train loss: 2.383154\n",
      "==>>> epoch: 0, batch index: 10200, train loss: 2.268914\n",
      "==>>> epoch: 0, batch index: 10300, train loss: 2.352687\n",
      "==>>> epoch: 0, batch index: 10400, train loss: 2.342769\n",
      "==>>> epoch: 0, batch index: 10500, train loss: 2.320172\n",
      "==>>> epoch: 0, batch index: 10600, train loss: 2.352977\n",
      "==>>> epoch: 0, batch index: 10700, train loss: 2.349206\n",
      "==>>> epoch: 0, batch index: 10800, train loss: 2.396834\n",
      "==>>> epoch: 0, batch index: 10900, train loss: 2.304576\n",
      "==>>> epoch: 0, batch index: 11000, train loss: 2.421112\n",
      "==>>> epoch: 0, batch index: 11100, train loss: 2.329218\n",
      "==>>> epoch: 0, batch index: 11200, train loss: 2.256613\n",
      "==>>> epoch: 0, batch index: 11300, train loss: 2.279235\n",
      "==>>> epoch: 0, batch index: 11400, train loss: 2.324290\n",
      "==>>> epoch: 0, batch index: 11500, train loss: 2.283954\n",
      "==>>> epoch: 0, batch index: 11600, train loss: 2.304932\n",
      "==>>> epoch: 0, batch index: 11700, train loss: 2.357772\n",
      "==>>> epoch: 0, batch index: 11800, train loss: 2.368871\n",
      "==>>> epoch: 0, batch index: 11900, train loss: 2.298991\n",
      "==>>> epoch: 0, batch index: 12000, train loss: 2.322357\n",
      "==>>> epoch: 0, batch index: 12100, train loss: 2.393460\n",
      "==>>> epoch: 0, batch index: 12200, train loss: 2.351355\n",
      "==>>> epoch: 0, batch index: 12300, train loss: 2.222757\n",
      "==>>> epoch: 0, batch index: 12400, train loss: 2.345254\n",
      "==>>> epoch: 0, batch index: 12500, train loss: 2.355295\n",
      "==>>> epoch: 0, batch index: 12600, train loss: 2.296254\n",
      "==>>> epoch: 0, batch index: 12700, train loss: 2.343064\n",
      "==>>> epoch: 0, batch index: 12800, train loss: 2.321812\n",
      "==>>> epoch: 0, batch index: 12900, train loss: 2.294279\n",
      "==>>> epoch: 0, batch index: 13000, train loss: 2.332500\n",
      "==>>> epoch: 0, batch index: 13100, train loss: 2.359184\n",
      "==>>> epoch: 0, batch index: 13200, train loss: 2.363646\n",
      "==>>> epoch: 0, batch index: 13300, train loss: 2.319981\n",
      "==>>> epoch: 0, batch index: 13400, train loss: 2.369986\n",
      "==>>> epoch: 0, batch index: 13500, train loss: 2.514045\n",
      "==>>> epoch: 0, batch index: 13600, train loss: 2.294225\n",
      "==>>> epoch: 0, batch index: 13700, train loss: 2.304893\n",
      "==>>> epoch: 0, batch index: 13800, train loss: 2.269355\n",
      "==>>> epoch: 0, batch index: 13900, train loss: 2.178502\n",
      "==>>> epoch: 0, batch index: 14000, train loss: 2.399861\n",
      "==>>> epoch: 0, batch index: 14100, train loss: 2.382845\n",
      "==>>> epoch: 0, batch index: 14200, train loss: 2.269300\n",
      "==>>> epoch: 0, batch index: 14300, train loss: 2.338715\n",
      "==>>> epoch: 0, batch index: 14400, train loss: 2.362333\n",
      "==>>> epoch: 0, batch index: 14500, train loss: 2.305427\n",
      "==>>> epoch: 0, batch index: 14600, train loss: 2.368305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 14700, train loss: 2.361027\n",
      "==>>> epoch: 0, batch index: 14800, train loss: 2.283446\n",
      "==>>> epoch: 0, batch index: 14900, train loss: 2.387924\n",
      "==>>> epoch: 0, batch index: 15000, train loss: 2.339913\n",
      "==>>> epoch: 0, batch index: 15100, train loss: 2.340443\n",
      "==>>> epoch: 0, batch index: 15200, train loss: 2.395314\n",
      "==>>> epoch: 0, batch index: 15300, train loss: 2.316582\n",
      "==>>> epoch: 0, batch index: 15400, train loss: 2.374598\n",
      "==>>> epoch: 0, batch index: 15500, train loss: 2.311883\n",
      "==>>> epoch: 0, batch index: 15600, train loss: 2.336558\n",
      "==>>> epoch: 0, batch index: 15700, train loss: 2.315594\n",
      "==>>> epoch: 0, batch index: 15800, train loss: 2.242960\n",
      "==>>> epoch: 0, batch index: 15900, train loss: 2.224170\n",
      "==>>> epoch: 0, batch index: 16000, train loss: 2.329422\n",
      "==>>> epoch: 0, batch index: 16100, train loss: 2.325356\n",
      "==>>> epoch: 0, batch index: 16200, train loss: 2.314312\n",
      "==>>> epoch: 0, batch index: 16300, train loss: 2.248769\n",
      "==>>> epoch: 0, batch index: 16400, train loss: 2.249861\n",
      "==>>> epoch: 0, batch index: 16500, train loss: 2.303066\n",
      "==>>> epoch: 0, batch index: 16600, train loss: 2.317526\n",
      "==>>> epoch: 0, batch index: 16700, train loss: 2.411191\n",
      "==>>> epoch: 0, batch index: 16800, train loss: 2.275226\n",
      "==>>> epoch: 0, batch index: 16900, train loss: 2.339633\n",
      "==>>> epoch: 0, batch index: 17000, train loss: 2.448864\n",
      "==>>> epoch: 0, batch index: 17100, train loss: 2.292403\n",
      "==>>> epoch: 0, batch index: 17200, train loss: 2.369275\n",
      "==>>> epoch: 0, batch index: 17300, train loss: 2.247449\n",
      "==>>> epoch: 0, batch index: 17400, train loss: 2.346420\n",
      "==>>> epoch: 0, batch index: 17500, train loss: 2.397439\n",
      "==>>> epoch: 0, batch index: 17600, train loss: 2.362038\n",
      "==>>> epoch: 0, batch index: 17700, train loss: 2.313616\n",
      "==>>> epoch: 0, batch index: 17800, train loss: 2.325564\n",
      "==>>> epoch: 0, batch index: 17900, train loss: 2.348269\n",
      "==>>> epoch: 0, batch index: 18000, train loss: 2.269641\n",
      "==>>> epoch: 0, batch index: 18100, train loss: 2.316342\n",
      "==>>> epoch: 0, batch index: 18200, train loss: 2.284567\n",
      "==>>> epoch: 0, batch index: 18300, train loss: 2.328275\n",
      "==>>> epoch: 0, batch index: 18400, train loss: 2.313627\n",
      "==>>> epoch: 0, batch index: 18500, train loss: 2.278317\n",
      "==>>> epoch: 0, batch index: 18600, train loss: 2.391052\n",
      "==>>> epoch: 0, batch index: 18700, train loss: 2.298787\n",
      "==>>> epoch: 0, batch index: 18800, train loss: 2.304160\n",
      "==>>> epoch: 0, batch index: 18900, train loss: 2.411046\n",
      "==>>> epoch: 0, batch index: 19000, train loss: 2.421728\n",
      "==>>> epoch: 0, batch index: 19100, train loss: 2.270089\n",
      "==>>> epoch: 0, batch index: 19200, train loss: 2.318254\n",
      "==>>> epoch: 0, batch index: 19300, train loss: 2.185692\n",
      "==>>> epoch: 0, batch index: 19400, train loss: 2.364208\n",
      "==>>> epoch: 0, batch index: 19500, train loss: 2.276729\n",
      "==>>> epoch: 0, batch index: 19600, train loss: 2.307909\n",
      "==>>> epoch: 0, batch index: 19700, train loss: 2.418311\n",
      "==>>> epoch: 0, batch index: 19800, train loss: 2.313484\n",
      "==>>> epoch: 0, batch index: 19900, train loss: 2.318551\n",
      "==>>> epoch: 0, batch index: 20000, train loss: 2.312184\n",
      "==>>> epoch: 0, batch index: 20100, train loss: 2.361369\n",
      "==>>> epoch: 0, batch index: 20200, train loss: 2.310615\n",
      "==>>> epoch: 0, batch index: 20300, train loss: 2.368998\n",
      "==>>> epoch: 0, batch index: 20400, train loss: 2.326446\n",
      "==>>> epoch: 0, batch index: 20500, train loss: 2.288497\n",
      "==>>> epoch: 0, batch index: 20600, train loss: 2.424011\n",
      "==>>> epoch: 0, batch index: 20700, train loss: 2.293153\n",
      "==>>> epoch: 0, batch index: 20800, train loss: 2.417804\n",
      "==>>> epoch: 0, batch index: 20900, train loss: 2.332391\n",
      "==>>> epoch: 0, batch index: 21000, train loss: 2.359315\n",
      "==>>> epoch: 0, batch index: 21100, train loss: 2.324106\n",
      "==>>> epoch: 0, batch index: 21200, train loss: 2.360927\n",
      "==>>> epoch: 0, batch index: 21300, train loss: 2.345416\n",
      "==>>> epoch: 0, batch index: 21400, train loss: 2.311065\n",
      "==>>> epoch: 0, batch index: 21500, train loss: 2.232032\n",
      "==>>> epoch: 0, batch index: 21600, train loss: 2.375411\n",
      "==>>> epoch: 0, batch index: 21700, train loss: 2.270015\n",
      "==>>> epoch: 0, batch index: 21800, train loss: 2.329784\n",
      "==>>> epoch: 0, batch index: 21900, train loss: 2.337211\n",
      "==>>> epoch: 0, batch index: 22000, train loss: 2.361883\n",
      "==>>> epoch: 0, batch index: 22100, train loss: 2.321468\n",
      "==>>> epoch: 0, batch index: 22200, train loss: 2.287387\n",
      "==>>> epoch: 0, batch index: 22300, train loss: 2.310169\n",
      "==>>> epoch: 0, batch index: 22400, train loss: 2.251570\n",
      "==>>> epoch: 0, batch index: 22500, train loss: 2.335526\n",
      "==>>> epoch: 0, batch index: 22600, train loss: 2.264856\n",
      "==>>> epoch: 0, batch index: 22700, train loss: 2.331633\n",
      "==>>> epoch: 0, batch index: 22800, train loss: 2.335752\n",
      "==>>> epoch: 0, batch index: 22900, train loss: 2.325139\n",
      "==>>> epoch: 0, batch index: 23000, train loss: 2.269481\n",
      "==>>> epoch: 0, batch index: 23100, train loss: 2.319314\n",
      "==>>> epoch: 0, batch index: 23200, train loss: 2.312397\n",
      "==>>> epoch: 0, batch index: 23300, train loss: 2.233163\n",
      "==>>> epoch: 0, batch index: 23400, train loss: 2.285593\n",
      "==>>> epoch: 0, batch index: 23500, train loss: 2.293649\n",
      "==>>> epoch: 0, batch index: 23600, train loss: 2.412468\n",
      "==>>> epoch: 0, batch index: 23700, train loss: 2.338738\n",
      "==>>> epoch: 0, batch index: 23800, train loss: 2.398451\n",
      "==>>> epoch: 0, batch index: 23900, train loss: 2.304176\n",
      "==>>> epoch: 0, batch index: 24000, train loss: 2.335505\n",
      "==>>> epoch: 0, batch index: 24100, train loss: 2.271675\n",
      "==>>> epoch: 0, batch index: 24200, train loss: 2.383155\n",
      "==>>> epoch: 0, batch index: 24300, train loss: 2.297977\n",
      "==>>> epoch: 0, batch index: 24400, train loss: 2.324871\n",
      "==>>> epoch: 0, batch index: 24500, train loss: 2.294076\n",
      "==>>> epoch: 0, batch index: 24600, train loss: 2.296992\n",
      "==>>> epoch: 0, batch index: 24700, train loss: 2.311284\n",
      "==>>> epoch: 0, batch index: 24800, train loss: 2.362164\n",
      "==>>> epoch: 0, batch index: 24900, train loss: 2.271256\n",
      "==>>> epoch: 0, batch index: 25000, train loss: 2.322147\n",
      "==>>> epoch: 0, batch index: 25100, train loss: 2.353580\n",
      "==>>> epoch: 0, batch index: 25200, train loss: 2.295693\n",
      "==>>> epoch: 0, batch index: 25300, train loss: 2.393927\n",
      "==>>> epoch: 0, batch index: 25400, train loss: 2.325349\n",
      "==>>> epoch: 0, batch index: 25500, train loss: 2.333351\n",
      "==>>> epoch: 0, batch index: 25600, train loss: 2.369484\n",
      "==>>> epoch: 0, batch index: 25700, train loss: 2.368316\n",
      "==>>> epoch: 0, batch index: 25800, train loss: 2.296824\n",
      "==>>> epoch: 0, batch index: 25900, train loss: 2.314955\n",
      "==>>> epoch: 0, batch index: 26000, train loss: 2.303245\n",
      "==>>> epoch: 0, batch index: 26100, train loss: 2.361872\n",
      "==>>> epoch: 0, batch index: 26200, train loss: 2.311199\n",
      "==>>> epoch: 0, batch index: 26300, train loss: 2.289531\n",
      "==>>> epoch: 0, batch index: 26400, train loss: 2.376715\n",
      "==>>> epoch: 0, batch index: 26500, train loss: 2.325607\n",
      "==>>> epoch: 0, batch index: 26600, train loss: 2.301343\n",
      "==>>> epoch: 0, batch index: 26700, train loss: 2.281894\n",
      "==>>> epoch: 0, batch index: 26800, train loss: 2.297039\n",
      "==>>> epoch: 0, batch index: 26900, train loss: 2.345072\n",
      "==>>> epoch: 0, batch index: 27000, train loss: 2.332295\n",
      "==>>> epoch: 0, batch index: 27100, train loss: 2.329775\n",
      "==>>> epoch: 0, batch index: 27200, train loss: 2.245877\n",
      "==>>> epoch: 0, batch index: 27300, train loss: 2.346133\n",
      "==>>> epoch: 0, batch index: 27400, train loss: 2.319776\n",
      "==>>> epoch: 0, batch index: 27500, train loss: 2.349291\n",
      "==>>> epoch: 0, batch index: 27600, train loss: 2.300054\n",
      "==>>> epoch: 0, batch index: 27700, train loss: 2.343385\n",
      "==>>> epoch: 0, batch index: 27800, train loss: 2.372702\n",
      "==>>> epoch: 0, batch index: 27900, train loss: 2.403814\n",
      "==>>> epoch: 0, batch index: 28000, train loss: 2.314919\n",
      "==>>> epoch: 0, batch index: 28100, train loss: 2.363370\n",
      "==>>> epoch: 0, batch index: 28200, train loss: 2.266945\n",
      "==>>> epoch: 0, batch index: 28300, train loss: 2.297788\n",
      "==>>> epoch: 0, batch index: 28400, train loss: 2.339463\n",
      "==>>> epoch: 0, batch index: 28500, train loss: 2.374310\n",
      "==>>> epoch: 0, batch index: 28600, train loss: 2.430746\n",
      "==>>> epoch: 0, batch index: 28700, train loss: 2.289195\n",
      "==>>> epoch: 0, batch index: 28800, train loss: 2.381863\n",
      "==>>> epoch: 0, batch index: 28900, train loss: 2.365642\n",
      "==>>> epoch: 0, batch index: 29000, train loss: 2.253883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 29100, train loss: 2.276406\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## training\n",
    "model = LeNet()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    # training\n",
    "    ave_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_dloader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_dloader):\n",
    "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss))\n",
    "    # testing\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_dloader):\n",
    "        with torch.no_grad():\n",
    "            x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.data.size()[0]\n",
    "        correct_cnt += (pred_label == target.data).sum()\n",
    "        # smooth average\n",
    "        #ave_loss = ave_loss * 0.9 + loss.item() * 0.1\n",
    "        \n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_dloader):\n",
    "            print ('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
