{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough project/code architecture\n",
    "\n",
    "1. Load MNIST into some kind of native dataformat (i.e. not with a oneliner that gives you a pytorch dataset)\n",
    "1. Load entire MNIST into a dataset, then make that into a dataloader\n",
    "1. Train MNIST with pytorch.  make a note of loss and accuracy. You’ll need to choose an architecture, batch size and number of epochs to train for. \n",
    "    * Get architecture from pytorch docs. Should be simple. \n",
    "    * Number of epochs is “until results get as good as they are going to get”, i.e. by inspection.\n",
    "    * Batch size I got no idea. 1 is probably fine, but feel free to try others.\n",
    "1. Split MNIST into N groups randomly. Create N datasets. Train with federated.py. You should get roughly the final loss and accuracy you saw previously. \n",
    "    * Use the same architecture as before.\n",
    "    * Choose number of rounds same way you chose number of epochs in previous step, i.e. until it converges.\n",
    "    * Number of epochs per round. Maybe try 1, 5 and 10? Go with 1 unless you get very different results.\n",
    "    * Batch size should probably be the same as before.\n",
    "1. Repeat, but split MNIST into N groups with the deck stacked (possibly as extremely as 10 groups each containing data from only one class).\n",
    "\n",
    "### Snippets\n",
    "\n",
    "#### Visualize an array\n",
    "See cell 109 here: https://github.com/williamsmj/pytorch-notes/blob/master/pytorch-60-minute-blitz.ipynb\n",
    "\n",
    "#### Change native data to dataset, change dataset to native data, and loop over dataset doing something other than training\n",
    "See https://github.com/williamsmj/pytorch-notes/blob/master/dataset_manipulation.ipynb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './MNIST-data/raw'\n",
    "\n",
    "# location of data and labels\n",
    "test_labels_file = data_path + '/' + 't10k-labels-idx1-ubyte'\n",
    "test_data_file = data_path + '/' + 't10k-images-idx3-ubyte'\n",
    "train_labels_file = data_path + '/' + 'train-labels-idx1-ubyte'\n",
    "train_data_file = data_path + '/' + 'train-images-idx3-ubyte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# pytorch datasets that download MNIST set as needed; used only to download files\n",
    "train_dset = dsets.MNIST(root=data_path, download=False, train=True, transform=trans)\n",
    "test_dset = dsets.MNIST(root=data_path, download=False, train=False, transform=trans)\n",
    "\n",
    "#print(\"Training dset:\", train_dset)\n",
    "#print(\"Test dset:\", test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data into numpy arrays\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "    \n",
    "# read data and label files into numpy arrays\n",
    "test_data = read_idx(test_data_file)\n",
    "test_labels = read_idx(test_labels_file)\n",
    "train_data = read_idx(train_data_file)\n",
    "train_labels = read_idx(train_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnp.savetxt(\"./MNIST-data/raw/test_data.csv\", test_data.reshape(len(test_data), 784), delimiter=\\',\\')\\nnp.savetxt(\"./MNIST-data/raw/test_labels.csv\", test_labels, delimiter=\\',\\')\\nnp.savetxt(\"./MNIST-data/raw/train_data.csv\", train_data.reshape(len(train_data), 784), delimiter=\\',\\')\\nnp.savetxt(\"./MNIST-data/raw/train_labels.csv\", train_labels, delimiter=\\',\\')\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save CSV files if needed\n",
    "\n",
    "\"\"\"\n",
    "np.savetxt(\"./MNIST-data/raw/test_data.csv\", test_data.reshape(len(test_data), 784), delimiter=',')\n",
    "np.savetxt(\"./MNIST-data/raw/test_labels.csv\", test_labels, delimiter=',')\n",
    "np.savetxt(\"./MNIST-data/raw/train_data.csv\", train_data.reshape(len(train_data), 784), delimiter=',')\n",
    "np.savetxt(\"./MNIST-data/raw/train_labels.csv\", train_labels, delimiter=',')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# to restore\n",
    "#np.loadtext()\n",
    "#reshape((len(), 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# pick a random sample from the sets of observations\\nimport random\\ntest_sample_num = random.randint(0, len(test_labels))\\ntrain_sample_num = random.randint(0, len(train_labels))\\n\\n# print some arrays to confirm we have data\\nprint(\"Test labels:\", test_labels)\\nprint(\"Test data:\", test_data[test_sample_num])\\nprint(\"Training labels:\", train_labels)\\nprint(\"Training data:\", train_data[train_sample_num])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the data in the arrays\n",
    "\n",
    "\"\"\"\n",
    "# pick a random sample from the sets of observations\n",
    "import random\n",
    "test_sample_num = random.randint(0, len(test_labels))\n",
    "train_sample_num = random.randint(0, len(train_labels))\n",
    "\n",
    "# print some arrays to confirm we have data\n",
    "print(\"Test labels:\", test_labels)\n",
    "print(\"Test data:\", test_data[test_sample_num])\n",
    "print(\"Training labels:\", train_labels)\n",
    "print(\"Training data:\", train_data[train_sample_num])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nimg = test_data[test_sample_num]\\nlabel = test_labels[test_sample_num]\\nprint('Test sample index: ', test_sample_num)\\nprint('Sample value: ', label)\\nplt.imshow(img)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize some test samples\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = test_data[test_sample_num]\n",
    "label = test_labels[test_sample_num]\n",
    "print('Test sample index: ', test_sample_num)\n",
    "print('Sample value: ', label)\n",
    "plt.imshow(img)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Test sample histogram:')\\nplt.hist(test_labels, bins=10)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how is the test data distributed?\n",
    "\n",
    "\"\"\"\n",
    "print('Test sample histogram:')\n",
    "plt.hist(test_labels, bins=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimg = train_data[train_sample_num]\\nlabel = train_labels[train_sample_num]\\nprint('Training sample index: ', train_sample_num)\\nprint('Sample value: ', label)\\nplt.imshow(img)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize a training sample\n",
    "\n",
    "\"\"\"\n",
    "img = train_data[train_sample_num]\n",
    "label = train_labels[train_sample_num]\n",
    "print('Training sample index: ', train_sample_num)\n",
    "print('Sample value: ', label)\n",
    "plt.imshow(img)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Training data histogram:')\\nplt.hist(train_labels, bins=10)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how is the training data distributed?\n",
    "\n",
    "\"\"\"\n",
    "print('Training data histogram:')\n",
    "plt.hist(train_labels, bins=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-58ba15a9c446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# load tensors into datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtest_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtrain_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "# load numpy arrays into pytorch Datasets\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# create tensors from np arrays\n",
    "test_data_tensor = torch.from_numpy(test_data)\n",
    "test_labels_tensor = torch.from_numpy(test_labels)\n",
    "train_data_tensor = torch.from_numpy(train_data)\n",
    "train_labels_tensor = torch.from_numpy(train_labels)\n",
    "\n",
    "\n",
    "# TODO: TRANSFORM THE DATA WHEN THE SETS ARE CREATED\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "\"\"\"\n",
    "\n",
    "# load tensors into datasets\n",
    "test_dset = TensorDataset(test_data_tensor, test_labels_tensor, transform=trans)\n",
    "train_dset = TensorDataset(train_data_tensor, train_labels_tensor, transform=trans)\n",
    "\n",
    "print(\"test_dset:\",\n",
    "      test_dset,\n",
    "      len(test_dset), test_dset[0][0].size(), type(test_dset[0][0])\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dloader 10000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 1\n",
    "test_dloader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)\n",
    "train_dloader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"test_dloader\", len(test_dloader),\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up network\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "## network\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MLP\"\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"LeNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 2000, train loss: 2.258546\n",
      "==>>> epoch: 0, batch index: 4000, train loss: 2.162534\n",
      "==>>> epoch: 0, batch index: 6000, train loss: 1.975261\n",
      "==>>> epoch: 0, batch index: 8000, train loss: 1.690423\n",
      "==>>> epoch: 0, batch index: 10000, train loss: 1.239327\n",
      "==>>> epoch: 0, batch index: 12000, train loss: 0.952410\n",
      "==>>> epoch: 0, batch index: 14000, train loss: 0.702495\n",
      "==>>> epoch: 0, batch index: 16000, train loss: 0.687791\n",
      "==>>> epoch: 0, batch index: 18000, train loss: 0.598707\n",
      "==>>> epoch: 0, batch index: 20000, train loss: 0.640071\n",
      "==>>> epoch: 0, batch index: 22000, train loss: 0.482804\n",
      "==>>> epoch: 0, batch index: 24000, train loss: 0.615392\n",
      "==>>> epoch: 0, batch index: 26000, train loss: 0.546289\n",
      "==>>> epoch: 0, batch index: 28000, train loss: 0.437503\n",
      "==>>> epoch: 0, batch index: 30000, train loss: 0.390828\n",
      "==>>> epoch: 0, batch index: 32000, train loss: 0.476861\n",
      "==>>> epoch: 0, batch index: 34000, train loss: 0.347122\n",
      "==>>> epoch: 0, batch index: 36000, train loss: 0.483678\n",
      "==>>> epoch: 0, batch index: 38000, train loss: 0.438023\n",
      "==>>> epoch: 0, batch index: 40000, train loss: 0.362165\n",
      "==>>> epoch: 0, batch index: 42000, train loss: 0.334782\n",
      "==>>> epoch: 0, batch index: 44000, train loss: 0.329365\n",
      "==>>> epoch: 0, batch index: 46000, train loss: 0.378069\n",
      "==>>> epoch: 0, batch index: 48000, train loss: 0.312284\n",
      "==>>> epoch: 0, batch index: 50000, train loss: 0.373520\n",
      "==>>> epoch: 0, batch index: 52000, train loss: 0.327304\n",
      "==>>> epoch: 0, batch index: 54000, train loss: 0.368761\n",
      "==>>> epoch: 0, batch index: 56000, train loss: 0.333183\n",
      "==>>> epoch: 0, batch index: 58000, train loss: 0.367758\n",
      "==>>> epoch: 0, batch index: 60000, train loss: 0.430468\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "model = MLPNet()\n",
    "\n",
    "learning_rate = 0.0003\n",
    "momentum = 0.5\n",
    "num_epochs = 1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = 0\n",
    "\n",
    "    # train\n",
    "    for batch_idx, (x, target) in enumerate(train_dloader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        train_loss_history.append(loss.item())\n",
    "        avg_loss = avg_loss * 0.99 + loss.item() * 0.01\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                    \n",
    "        if (batch_idx+1) % 2000 == 0 or (batch_idx+1) == len(train_dloader):\n",
    "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, avg_loss))\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# ADD TESTING AFTER TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD PLOTTING\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss_history, label=\"train\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 100, train loss: 1.464695\n",
      "==>>> epoch: 0, batch index: 200, train loss: 1.747897\n",
      "==>>> epoch: 0, batch index: 300, train loss: 1.791303\n",
      "==>>> epoch: 0, batch index: 400, train loss: 1.883426\n",
      "==>>> epoch: 0, batch index: 500, train loss: 1.700222\n",
      "==>>> epoch: 0, batch index: 600, train loss: 1.842954\n",
      "==>>> epoch: 0, batch index: 700, train loss: 1.886750\n",
      "==>>> epoch: 0, batch index: 800, train loss: 2.018955\n",
      "==>>> epoch: 0, batch index: 900, train loss: 1.843694\n",
      "==>>> epoch: 0, batch index: 1000, train loss: 1.896781\n",
      "==>>> epoch: 0, batch index: 1100, train loss: 1.761072\n",
      "==>>> epoch: 0, batch index: 1200, train loss: 1.853076\n",
      "==>>> epoch: 0, batch index: 1300, train loss: 1.539231\n",
      "==>>> epoch: 0, batch index: 1400, train loss: 1.510832\n",
      "==>>> epoch: 0, batch index: 1500, train loss: 1.398014\n",
      "==>>> epoch: 0, batch index: 1600, train loss: 1.803469\n",
      "==>>> epoch: 0, batch index: 1700, train loss: 2.165218\n",
      "==>>> epoch: 0, batch index: 1800, train loss: 2.254305\n",
      "==>>> epoch: 0, batch index: 1900, train loss: 2.327984\n",
      "==>>> epoch: 0, batch index: 2000, train loss: 2.323085\n",
      "==>>> epoch: 0, batch index: 2100, train loss: 2.318849\n",
      "==>>> epoch: 0, batch index: 2200, train loss: 2.325567\n",
      "==>>> epoch: 0, batch index: 2300, train loss: 2.324261\n",
      "==>>> epoch: 0, batch index: 2400, train loss: 2.323766\n",
      "==>>> epoch: 0, batch index: 2500, train loss: 2.325214\n",
      "==>>> epoch: 0, batch index: 2600, train loss: 2.337289\n",
      "==>>> epoch: 0, batch index: 2700, train loss: 2.336839\n",
      "==>>> epoch: 0, batch index: 2800, train loss: 2.334101\n",
      "==>>> epoch: 0, batch index: 2900, train loss: 2.343406\n",
      "==>>> epoch: 0, batch index: 3000, train loss: 2.331263\n",
      "==>>> epoch: 0, batch index: 3100, train loss: 2.332626\n",
      "==>>> epoch: 0, batch index: 3200, train loss: 2.300122\n",
      "==>>> epoch: 0, batch index: 3300, train loss: 2.306233\n",
      "==>>> epoch: 0, batch index: 3400, train loss: 2.331318\n",
      "==>>> epoch: 0, batch index: 3500, train loss: 2.306865\n",
      "==>>> epoch: 0, batch index: 3600, train loss: 2.304600\n",
      "==>>> epoch: 0, batch index: 3700, train loss: 2.338269\n",
      "==>>> epoch: 0, batch index: 3800, train loss: 2.333785\n",
      "==>>> epoch: 0, batch index: 3900, train loss: 2.316661\n",
      "==>>> epoch: 0, batch index: 4000, train loss: 2.315973\n",
      "==>>> epoch: 0, batch index: 4100, train loss: 2.332038\n",
      "==>>> epoch: 0, batch index: 4200, train loss: 2.332153\n",
      "==>>> epoch: 0, batch index: 4300, train loss: 2.318826\n",
      "==>>> epoch: 0, batch index: 4400, train loss: 2.308681\n",
      "==>>> epoch: 0, batch index: 4500, train loss: 2.322079\n",
      "==>>> epoch: 0, batch index: 4600, train loss: 2.327692\n",
      "==>>> epoch: 0, batch index: 4700, train loss: 2.322073\n",
      "==>>> epoch: 0, batch index: 4800, train loss: 2.298684\n",
      "==>>> epoch: 0, batch index: 4900, train loss: 2.285471\n",
      "==>>> epoch: 0, batch index: 5000, train loss: 2.331332\n",
      "==>>> epoch: 0, batch index: 5100, train loss: 2.322183\n",
      "==>>> epoch: 0, batch index: 5200, train loss: 2.283995\n",
      "==>>> epoch: 0, batch index: 5300, train loss: 2.315028\n",
      "==>>> epoch: 0, batch index: 5400, train loss: 2.333618\n",
      "==>>> epoch: 0, batch index: 5500, train loss: 2.315576\n",
      "==>>> epoch: 0, batch index: 5600, train loss: 2.330427\n",
      "==>>> epoch: 0, batch index: 5700, train loss: 2.326331\n",
      "==>>> epoch: 0, batch index: 5800, train loss: 2.295223\n",
      "==>>> epoch: 0, batch index: 5900, train loss: 2.314463\n",
      "==>>> epoch: 0, batch index: 6000, train loss: 2.328956\n",
      "==>>> epoch: 0, batch index: 6100, train loss: 2.329368\n",
      "==>>> epoch: 0, batch index: 6200, train loss: 2.305900\n",
      "==>>> epoch: 0, batch index: 6300, train loss: 2.311567\n",
      "==>>> epoch: 0, batch index: 6400, train loss: 2.312481\n",
      "==>>> epoch: 0, batch index: 6500, train loss: 2.327387\n",
      "==>>> epoch: 0, batch index: 6600, train loss: 2.314831\n",
      "==>>> epoch: 0, batch index: 6700, train loss: 2.329899\n",
      "==>>> epoch: 0, batch index: 6800, train loss: 2.329994\n",
      "==>>> epoch: 0, batch index: 6900, train loss: 2.334922\n",
      "==>>> epoch: 0, batch index: 7000, train loss: 2.326186\n",
      "==>>> epoch: 0, batch index: 7100, train loss: 2.333560\n",
      "==>>> epoch: 0, batch index: 7200, train loss: 2.320925\n",
      "==>>> epoch: 0, batch index: 7300, train loss: 2.330711\n",
      "==>>> epoch: 0, batch index: 7400, train loss: 2.330569\n",
      "==>>> epoch: 0, batch index: 7500, train loss: 2.317566\n",
      "==>>> epoch: 0, batch index: 7600, train loss: 2.320108\n",
      "==>>> epoch: 0, batch index: 7700, train loss: 2.327094\n",
      "==>>> epoch: 0, batch index: 7800, train loss: 2.304519\n",
      "==>>> epoch: 0, batch index: 7900, train loss: 2.327349\n",
      "==>>> epoch: 0, batch index: 8000, train loss: 2.320627\n",
      "==>>> epoch: 0, batch index: 8100, train loss: 2.324323\n",
      "==>>> epoch: 0, batch index: 8200, train loss: 2.334843\n",
      "==>>> epoch: 0, batch index: 8300, train loss: 2.322516\n",
      "==>>> epoch: 0, batch index: 8400, train loss: 2.320856\n",
      "==>>> epoch: 0, batch index: 8500, train loss: 2.340906\n",
      "==>>> epoch: 0, batch index: 8600, train loss: 2.320203\n",
      "==>>> epoch: 0, batch index: 8700, train loss: 2.323318\n",
      "==>>> epoch: 0, batch index: 8800, train loss: 2.336748\n",
      "==>>> epoch: 0, batch index: 8900, train loss: 2.330275\n",
      "==>>> epoch: 0, batch index: 9000, train loss: 2.340644\n",
      "==>>> epoch: 0, batch index: 9100, train loss: 2.324306\n",
      "==>>> epoch: 0, batch index: 9200, train loss: 2.345746\n",
      "==>>> epoch: 0, batch index: 9300, train loss: 2.331159\n",
      "==>>> epoch: 0, batch index: 9400, train loss: 2.331862\n",
      "==>>> epoch: 0, batch index: 9500, train loss: 2.338108\n",
      "==>>> epoch: 0, batch index: 9600, train loss: 2.317460\n",
      "==>>> epoch: 0, batch index: 9700, train loss: 2.303056\n",
      "==>>> epoch: 0, batch index: 9800, train loss: 2.337827\n",
      "==>>> epoch: 0, batch index: 9900, train loss: 2.323902\n",
      "==>>> epoch: 0, batch index: 10000, train loss: 2.326517\n",
      "==>>> epoch: 0, batch index: 10100, train loss: 2.320073\n",
      "==>>> epoch: 0, batch index: 10200, train loss: 2.327928\n",
      "==>>> epoch: 0, batch index: 10300, train loss: 2.313289\n",
      "==>>> epoch: 0, batch index: 10400, train loss: 2.321256\n",
      "==>>> epoch: 0, batch index: 10500, train loss: 2.300740\n",
      "==>>> epoch: 0, batch index: 10600, train loss: 2.333683\n",
      "==>>> epoch: 0, batch index: 10700, train loss: 2.320705\n",
      "==>>> epoch: 0, batch index: 10800, train loss: 2.298117\n",
      "==>>> epoch: 0, batch index: 10900, train loss: 2.341593\n",
      "==>>> epoch: 0, batch index: 11000, train loss: 2.334929\n",
      "==>>> epoch: 0, batch index: 11100, train loss: 2.302443\n",
      "==>>> epoch: 0, batch index: 11200, train loss: 2.333573\n",
      "==>>> epoch: 0, batch index: 11300, train loss: 2.320770\n",
      "==>>> epoch: 0, batch index: 11400, train loss: 2.298591\n",
      "==>>> epoch: 0, batch index: 11500, train loss: 2.305519\n",
      "==>>> epoch: 0, batch index: 11600, train loss: 2.341044\n",
      "==>>> epoch: 0, batch index: 11700, train loss: 2.328075\n",
      "==>>> epoch: 0, batch index: 11800, train loss: 2.326959\n",
      "==>>> epoch: 0, batch index: 11900, train loss: 2.333364\n",
      "==>>> epoch: 0, batch index: 12000, train loss: 2.315151\n",
      "==>>> epoch: 0, batch index: 12100, train loss: 2.311859\n",
      "==>>> epoch: 0, batch index: 12200, train loss: 2.323341\n",
      "==>>> epoch: 0, batch index: 12300, train loss: 2.318833\n",
      "==>>> epoch: 0, batch index: 12400, train loss: 2.313243\n",
      "==>>> epoch: 0, batch index: 12500, train loss: 2.329862\n",
      "==>>> epoch: 0, batch index: 12600, train loss: 2.328715\n",
      "==>>> epoch: 0, batch index: 12700, train loss: 2.342821\n",
      "==>>> epoch: 0, batch index: 12800, train loss: 2.335909\n",
      "==>>> epoch: 0, batch index: 12900, train loss: 2.294916\n",
      "==>>> epoch: 0, batch index: 13000, train loss: 2.319011\n",
      "==>>> epoch: 0, batch index: 13100, train loss: 2.311767\n",
      "==>>> epoch: 0, batch index: 13200, train loss: 2.328494\n",
      "==>>> epoch: 0, batch index: 13300, train loss: 2.330140\n",
      "==>>> epoch: 0, batch index: 13400, train loss: 2.321666\n",
      "==>>> epoch: 0, batch index: 13500, train loss: 2.326810\n",
      "==>>> epoch: 0, batch index: 13600, train loss: 2.329577\n",
      "==>>> epoch: 0, batch index: 13700, train loss: 2.314335\n",
      "==>>> epoch: 0, batch index: 13800, train loss: 2.333965\n",
      "==>>> epoch: 0, batch index: 13900, train loss: 2.321853\n",
      "==>>> epoch: 0, batch index: 14000, train loss: 2.326159\n",
      "==>>> epoch: 0, batch index: 14100, train loss: 2.319235\n",
      "==>>> epoch: 0, batch index: 14200, train loss: 2.281469\n",
      "==>>> epoch: 0, batch index: 14300, train loss: 2.327507\n",
      "==>>> epoch: 0, batch index: 14400, train loss: 2.314128\n",
      "==>>> epoch: 0, batch index: 14500, train loss: 2.327667\n",
      "==>>> epoch: 0, batch index: 14600, train loss: 2.305368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 14700, train loss: 2.316671\n",
      "==>>> epoch: 0, batch index: 14800, train loss: 2.309757\n",
      "==>>> epoch: 0, batch index: 14900, train loss: 2.324306\n",
      "==>>> epoch: 0, batch index: 15000, train loss: 2.311145\n",
      "==>>> epoch: 0, batch index: 15100, train loss: 2.307342\n",
      "==>>> epoch: 0, batch index: 15200, train loss: 2.328685\n",
      "==>>> epoch: 0, batch index: 15300, train loss: 2.314320\n",
      "==>>> epoch: 0, batch index: 15400, train loss: 2.300478\n",
      "==>>> epoch: 0, batch index: 15500, train loss: 2.320795\n",
      "==>>> epoch: 0, batch index: 15600, train loss: 2.325919\n",
      "==>>> epoch: 0, batch index: 15700, train loss: 2.326250\n",
      "==>>> epoch: 0, batch index: 15800, train loss: 2.331537\n",
      "==>>> epoch: 0, batch index: 15900, train loss: 2.336182\n",
      "==>>> epoch: 0, batch index: 16000, train loss: 2.322068\n",
      "==>>> epoch: 0, batch index: 16100, train loss: 2.333179\n",
      "==>>> epoch: 0, batch index: 16200, train loss: 2.328012\n",
      "==>>> epoch: 0, batch index: 16300, train loss: 2.331956\n",
      "==>>> epoch: 0, batch index: 16400, train loss: 2.327839\n",
      "==>>> epoch: 0, batch index: 16500, train loss: 2.337245\n",
      "==>>> epoch: 0, batch index: 16600, train loss: 2.325754\n",
      "==>>> epoch: 0, batch index: 16700, train loss: 2.325244\n",
      "==>>> epoch: 0, batch index: 16800, train loss: 2.332517\n",
      "==>>> epoch: 0, batch index: 16900, train loss: 2.306323\n",
      "==>>> epoch: 0, batch index: 17000, train loss: 2.322386\n",
      "==>>> epoch: 0, batch index: 17100, train loss: 2.325984\n",
      "==>>> epoch: 0, batch index: 17200, train loss: 2.315103\n",
      "==>>> epoch: 0, batch index: 17300, train loss: 2.318924\n",
      "==>>> epoch: 0, batch index: 17400, train loss: 2.317744\n",
      "==>>> epoch: 0, batch index: 17500, train loss: 2.336563\n",
      "==>>> epoch: 0, batch index: 17600, train loss: 2.337245\n",
      "==>>> epoch: 0, batch index: 17700, train loss: 2.325998\n",
      "==>>> epoch: 0, batch index: 17800, train loss: 2.325154\n",
      "==>>> epoch: 0, batch index: 17900, train loss: 2.304553\n",
      "==>>> epoch: 0, batch index: 18000, train loss: 2.303707\n",
      "==>>> epoch: 0, batch index: 18100, train loss: 2.327159\n",
      "==>>> epoch: 0, batch index: 18200, train loss: 2.338385\n",
      "==>>> epoch: 0, batch index: 18300, train loss: 2.337766\n",
      "==>>> epoch: 0, batch index: 18400, train loss: 2.322560\n",
      "==>>> epoch: 0, batch index: 18500, train loss: 2.320826\n",
      "==>>> epoch: 0, batch index: 18600, train loss: 2.319830\n",
      "==>>> epoch: 0, batch index: 18700, train loss: 2.316015\n",
      "==>>> epoch: 0, batch index: 18800, train loss: 2.330892\n",
      "==>>> epoch: 0, batch index: 18900, train loss: 2.341754\n",
      "==>>> epoch: 0, batch index: 19000, train loss: 2.322575\n",
      "==>>> epoch: 0, batch index: 19100, train loss: 2.323068\n",
      "==>>> epoch: 0, batch index: 19200, train loss: 2.331280\n",
      "==>>> epoch: 0, batch index: 19300, train loss: 2.340030\n",
      "==>>> epoch: 0, batch index: 19400, train loss: 2.328072\n",
      "==>>> epoch: 0, batch index: 19500, train loss: 2.307932\n",
      "==>>> epoch: 0, batch index: 19600, train loss: 2.310291\n",
      "==>>> epoch: 0, batch index: 19700, train loss: 2.323726\n",
      "==>>> epoch: 0, batch index: 19800, train loss: 2.317583\n",
      "==>>> epoch: 0, batch index: 19900, train loss: 2.309494\n",
      "==>>> epoch: 0, batch index: 20000, train loss: 2.291943\n",
      "==>>> epoch: 0, batch index: 20100, train loss: 2.322296\n",
      "==>>> epoch: 0, batch index: 20200, train loss: 2.331397\n",
      "==>>> epoch: 0, batch index: 20300, train loss: 2.333195\n",
      "==>>> epoch: 0, batch index: 20400, train loss: 2.345639\n",
      "==>>> epoch: 0, batch index: 20500, train loss: 2.325138\n",
      "==>>> epoch: 0, batch index: 20600, train loss: 2.283494\n",
      "==>>> epoch: 0, batch index: 20700, train loss: 2.309410\n",
      "==>>> epoch: 0, batch index: 20800, train loss: 2.321704\n",
      "==>>> epoch: 0, batch index: 20900, train loss: 2.344217\n",
      "==>>> epoch: 0, batch index: 21000, train loss: 2.311000\n",
      "==>>> epoch: 0, batch index: 21100, train loss: 2.332135\n",
      "==>>> epoch: 0, batch index: 21200, train loss: 2.339969\n",
      "==>>> epoch: 0, batch index: 21300, train loss: 2.337026\n",
      "==>>> epoch: 0, batch index: 21400, train loss: 2.306766\n",
      "==>>> epoch: 0, batch index: 21500, train loss: 2.328146\n",
      "==>>> epoch: 0, batch index: 21600, train loss: 2.310107\n",
      "==>>> epoch: 0, batch index: 21700, train loss: 2.312364\n",
      "==>>> epoch: 0, batch index: 21800, train loss: 2.333588\n",
      "==>>> epoch: 0, batch index: 21900, train loss: 2.308645\n",
      "==>>> epoch: 0, batch index: 22000, train loss: 2.344057\n",
      "==>>> epoch: 0, batch index: 22100, train loss: 2.325714\n",
      "==>>> epoch: 0, batch index: 22200, train loss: 2.313559\n",
      "==>>> epoch: 0, batch index: 22300, train loss: 2.328033\n",
      "==>>> epoch: 0, batch index: 22400, train loss: 2.322290\n",
      "==>>> epoch: 0, batch index: 22500, train loss: 2.308520\n",
      "==>>> epoch: 0, batch index: 22600, train loss: 2.323845\n",
      "==>>> epoch: 0, batch index: 22700, train loss: 2.323999\n",
      "==>>> epoch: 0, batch index: 22800, train loss: 2.329231\n",
      "==>>> epoch: 0, batch index: 22900, train loss: 2.330322\n",
      "==>>> epoch: 0, batch index: 23000, train loss: 2.328703\n",
      "==>>> epoch: 0, batch index: 23100, train loss: 2.321451\n",
      "==>>> epoch: 0, batch index: 23200, train loss: 2.309525\n",
      "==>>> epoch: 0, batch index: 23300, train loss: 2.337254\n",
      "==>>> epoch: 0, batch index: 23400, train loss: 2.328317\n",
      "==>>> epoch: 0, batch index: 23500, train loss: 2.318016\n",
      "==>>> epoch: 0, batch index: 23600, train loss: 2.338164\n",
      "==>>> epoch: 0, batch index: 23700, train loss: 2.328868\n",
      "==>>> epoch: 0, batch index: 23800, train loss: 2.319683\n",
      "==>>> epoch: 0, batch index: 23900, train loss: 2.313904\n",
      "==>>> epoch: 0, batch index: 24000, train loss: 2.332889\n",
      "==>>> epoch: 0, batch index: 24100, train loss: 2.332556\n",
      "==>>> epoch: 0, batch index: 24200, train loss: 2.306904\n",
      "==>>> epoch: 0, batch index: 24300, train loss: 2.337478\n",
      "==>>> epoch: 0, batch index: 24400, train loss: 2.340076\n",
      "==>>> epoch: 0, batch index: 24500, train loss: 2.311709\n",
      "==>>> epoch: 0, batch index: 24600, train loss: 2.331143\n",
      "==>>> epoch: 0, batch index: 24700, train loss: 2.337776\n",
      "==>>> epoch: 0, batch index: 24800, train loss: 2.308451\n",
      "==>>> epoch: 0, batch index: 24900, train loss: 2.325378\n",
      "==>>> epoch: 0, batch index: 25000, train loss: 2.319537\n",
      "==>>> epoch: 0, batch index: 25100, train loss: 2.320263\n",
      "==>>> epoch: 0, batch index: 25200, train loss: 2.328808\n",
      "==>>> epoch: 0, batch index: 25300, train loss: 2.316311\n",
      "==>>> epoch: 0, batch index: 25400, train loss: 2.328873\n",
      "==>>> epoch: 0, batch index: 25500, train loss: 2.327007\n",
      "==>>> epoch: 0, batch index: 25600, train loss: 2.333224\n",
      "==>>> epoch: 0, batch index: 25700, train loss: 2.319399\n",
      "==>>> epoch: 0, batch index: 25800, train loss: 2.321066\n",
      "==>>> epoch: 0, batch index: 25900, train loss: 2.316473\n",
      "==>>> epoch: 0, batch index: 26000, train loss: 2.321653\n",
      "==>>> epoch: 0, batch index: 26100, train loss: 2.336439\n",
      "==>>> epoch: 0, batch index: 26200, train loss: 2.297513\n",
      "==>>> epoch: 0, batch index: 26300, train loss: 2.339707\n",
      "==>>> epoch: 0, batch index: 26400, train loss: 2.327671\n",
      "==>>> epoch: 0, batch index: 26500, train loss: 2.317428\n",
      "==>>> epoch: 0, batch index: 26600, train loss: 2.332602\n",
      "==>>> epoch: 0, batch index: 26700, train loss: 2.338440\n",
      "==>>> epoch: 0, batch index: 26800, train loss: 2.332618\n",
      "==>>> epoch: 0, batch index: 26900, train loss: 2.319997\n",
      "==>>> epoch: 0, batch index: 27000, train loss: 2.309041\n",
      "==>>> epoch: 0, batch index: 27100, train loss: 2.327912\n",
      "==>>> epoch: 0, batch index: 27200, train loss: 2.319508\n",
      "==>>> epoch: 0, batch index: 27300, train loss: 2.338598\n",
      "==>>> epoch: 0, batch index: 27400, train loss: 2.317506\n",
      "==>>> epoch: 0, batch index: 27500, train loss: 2.340280\n",
      "==>>> epoch: 0, batch index: 27600, train loss: 2.321145\n",
      "==>>> epoch: 0, batch index: 27700, train loss: 2.317222\n",
      "==>>> epoch: 0, batch index: 27800, train loss: 2.342992\n",
      "==>>> epoch: 0, batch index: 27900, train loss: 2.328957\n",
      "==>>> epoch: 0, batch index: 28000, train loss: 2.314742\n",
      "==>>> epoch: 0, batch index: 28100, train loss: 2.323375\n",
      "==>>> epoch: 0, batch index: 28200, train loss: 2.311939\n",
      "==>>> epoch: 0, batch index: 28300, train loss: 2.319757\n",
      "==>>> epoch: 0, batch index: 28400, train loss: 2.326532\n",
      "==>>> epoch: 0, batch index: 28500, train loss: 2.334232\n",
      "==>>> epoch: 0, batch index: 28600, train loss: 2.320648\n",
      "==>>> epoch: 0, batch index: 28700, train loss: 2.335282\n",
      "==>>> epoch: 0, batch index: 28800, train loss: 2.306928\n",
      "==>>> epoch: 0, batch index: 28900, train loss: 2.325414\n",
      "==>>> epoch: 0, batch index: 29000, train loss: 2.326090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 29100, train loss: 2.314239\n",
      "==>>> epoch: 0, batch index: 29200, train loss: 2.309528\n",
      "==>>> epoch: 0, batch index: 29300, train loss: 2.307035\n",
      "==>>> epoch: 0, batch index: 29400, train loss: 2.323896\n",
      "==>>> epoch: 0, batch index: 29500, train loss: 2.326598\n",
      "==>>> epoch: 0, batch index: 29600, train loss: 2.329159\n",
      "==>>> epoch: 0, batch index: 29700, train loss: 2.320279\n",
      "==>>> epoch: 0, batch index: 29800, train loss: 2.336527\n",
      "==>>> epoch: 0, batch index: 29900, train loss: 2.343784\n",
      "==>>> epoch: 0, batch index: 30000, train loss: 2.335966\n",
      "==>>> epoch: 0, batch index: 30100, train loss: 2.338518\n",
      "==>>> epoch: 0, batch index: 30200, train loss: 2.331431\n",
      "==>>> epoch: 0, batch index: 30300, train loss: 2.339263\n",
      "==>>> epoch: 0, batch index: 30400, train loss: 2.334477\n",
      "==>>> epoch: 0, batch index: 30500, train loss: 2.319819\n",
      "==>>> epoch: 0, batch index: 30600, train loss: 2.327896\n",
      "==>>> epoch: 0, batch index: 30700, train loss: 2.333983\n",
      "==>>> epoch: 0, batch index: 30800, train loss: 2.330283\n",
      "==>>> epoch: 0, batch index: 30900, train loss: 2.301798\n",
      "==>>> epoch: 0, batch index: 31000, train loss: 2.331158\n",
      "==>>> epoch: 0, batch index: 31100, train loss: 2.326820\n",
      "==>>> epoch: 0, batch index: 31200, train loss: 2.320290\n",
      "==>>> epoch: 0, batch index: 31300, train loss: 2.331206\n",
      "==>>> epoch: 0, batch index: 31400, train loss: 2.335425\n",
      "==>>> epoch: 0, batch index: 31500, train loss: 2.329776\n",
      "==>>> epoch: 0, batch index: 31600, train loss: 2.316201\n",
      "==>>> epoch: 0, batch index: 31700, train loss: 2.321999\n",
      "==>>> epoch: 0, batch index: 31800, train loss: 2.325332\n",
      "==>>> epoch: 0, batch index: 31900, train loss: 2.314146\n",
      "==>>> epoch: 0, batch index: 32000, train loss: 2.337999\n",
      "==>>> epoch: 0, batch index: 32100, train loss: 2.319003\n",
      "==>>> epoch: 0, batch index: 32200, train loss: 2.322076\n",
      "==>>> epoch: 0, batch index: 32300, train loss: 2.304092\n",
      "==>>> epoch: 0, batch index: 32400, train loss: 2.326787\n",
      "==>>> epoch: 0, batch index: 32500, train loss: 2.347723\n",
      "==>>> epoch: 0, batch index: 32600, train loss: 2.338921\n",
      "==>>> epoch: 0, batch index: 32700, train loss: 2.324893\n",
      "==>>> epoch: 0, batch index: 32800, train loss: 2.339671\n",
      "==>>> epoch: 0, batch index: 32900, train loss: 2.329776\n",
      "==>>> epoch: 0, batch index: 33000, train loss: 2.332287\n",
      "==>>> epoch: 0, batch index: 33100, train loss: 2.332914\n",
      "==>>> epoch: 0, batch index: 33200, train loss: 2.322928\n",
      "==>>> epoch: 0, batch index: 33300, train loss: 2.333992\n",
      "==>>> epoch: 0, batch index: 33400, train loss: 2.324845\n",
      "==>>> epoch: 0, batch index: 33500, train loss: 2.318245\n",
      "==>>> epoch: 0, batch index: 33600, train loss: 2.329936\n",
      "==>>> epoch: 0, batch index: 33700, train loss: 2.341856\n",
      "==>>> epoch: 0, batch index: 33800, train loss: 2.334833\n",
      "==>>> epoch: 0, batch index: 33900, train loss: 2.345853\n",
      "==>>> epoch: 0, batch index: 34000, train loss: 2.333937\n",
      "==>>> epoch: 0, batch index: 34100, train loss: 2.327744\n",
      "==>>> epoch: 0, batch index: 34200, train loss: 2.317886\n",
      "==>>> epoch: 0, batch index: 34300, train loss: 2.324268\n",
      "==>>> epoch: 0, batch index: 34400, train loss: 2.310556\n",
      "==>>> epoch: 0, batch index: 34500, train loss: 2.313995\n",
      "==>>> epoch: 0, batch index: 34600, train loss: 2.340818\n",
      "==>>> epoch: 0, batch index: 34700, train loss: 2.330406\n",
      "==>>> epoch: 0, batch index: 34800, train loss: 2.312235\n",
      "==>>> epoch: 0, batch index: 34900, train loss: 2.303815\n",
      "==>>> epoch: 0, batch index: 35000, train loss: 2.326633\n",
      "==>>> epoch: 0, batch index: 35100, train loss: 2.337447\n",
      "==>>> epoch: 0, batch index: 35200, train loss: 2.320927\n",
      "==>>> epoch: 0, batch index: 35300, train loss: 2.302348\n",
      "==>>> epoch: 0, batch index: 35400, train loss: 2.303792\n",
      "==>>> epoch: 0, batch index: 35500, train loss: 2.313298\n",
      "==>>> epoch: 0, batch index: 35600, train loss: 2.323379\n",
      "==>>> epoch: 0, batch index: 35700, train loss: 2.332380\n",
      "==>>> epoch: 0, batch index: 35800, train loss: 2.317283\n",
      "==>>> epoch: 0, batch index: 35900, train loss: 2.337113\n",
      "==>>> epoch: 0, batch index: 36000, train loss: 2.309108\n",
      "==>>> epoch: 0, batch index: 36100, train loss: 2.307246\n",
      "==>>> epoch: 0, batch index: 36200, train loss: 2.337297\n",
      "==>>> epoch: 0, batch index: 36300, train loss: 2.327497\n",
      "==>>> epoch: 0, batch index: 36400, train loss: 2.316861\n",
      "==>>> epoch: 0, batch index: 36500, train loss: 2.336166\n",
      "==>>> epoch: 0, batch index: 36600, train loss: 2.318969\n",
      "==>>> epoch: 0, batch index: 36700, train loss: 2.324365\n",
      "==>>> epoch: 0, batch index: 36800, train loss: 2.323376\n",
      "==>>> epoch: 0, batch index: 36900, train loss: 2.336217\n",
      "==>>> epoch: 0, batch index: 37000, train loss: 2.326539\n",
      "==>>> epoch: 0, batch index: 37100, train loss: 2.306288\n",
      "==>>> epoch: 0, batch index: 37200, train loss: 2.323926\n",
      "==>>> epoch: 0, batch index: 37300, train loss: 2.322698\n",
      "==>>> epoch: 0, batch index: 37400, train loss: 2.316217\n",
      "==>>> epoch: 0, batch index: 37500, train loss: 2.329692\n",
      "==>>> epoch: 0, batch index: 37600, train loss: 2.322848\n",
      "==>>> epoch: 0, batch index: 37700, train loss: 2.337808\n",
      "==>>> epoch: 0, batch index: 37800, train loss: 2.304487\n",
      "==>>> epoch: 0, batch index: 37900, train loss: 2.300600\n",
      "==>>> epoch: 0, batch index: 38000, train loss: 2.311475\n",
      "==>>> epoch: 0, batch index: 38100, train loss: 2.308495\n",
      "==>>> epoch: 0, batch index: 38200, train loss: 2.317909\n",
      "==>>> epoch: 0, batch index: 38300, train loss: 2.325252\n",
      "==>>> epoch: 0, batch index: 38400, train loss: 2.313844\n",
      "==>>> epoch: 0, batch index: 38500, train loss: 2.312909\n",
      "==>>> epoch: 0, batch index: 38600, train loss: 2.323123\n",
      "==>>> epoch: 0, batch index: 38700, train loss: 2.312400\n",
      "==>>> epoch: 0, batch index: 38800, train loss: 2.317434\n",
      "==>>> epoch: 0, batch index: 38900, train loss: 2.321198\n",
      "==>>> epoch: 0, batch index: 39000, train loss: 2.324537\n",
      "==>>> epoch: 0, batch index: 39100, train loss: 2.341825\n",
      "==>>> epoch: 0, batch index: 39200, train loss: 2.321513\n",
      "==>>> epoch: 0, batch index: 39300, train loss: 2.300540\n",
      "==>>> epoch: 0, batch index: 39400, train loss: 2.322518\n",
      "==>>> epoch: 0, batch index: 39500, train loss: 2.314285\n",
      "==>>> epoch: 0, batch index: 39600, train loss: 2.315603\n",
      "==>>> epoch: 0, batch index: 39700, train loss: 2.318949\n",
      "==>>> epoch: 0, batch index: 39800, train loss: 2.322011\n",
      "==>>> epoch: 0, batch index: 39900, train loss: 2.324466\n",
      "==>>> epoch: 0, batch index: 40000, train loss: 2.311675\n",
      "==>>> epoch: 0, batch index: 40100, train loss: 2.311706\n",
      "==>>> epoch: 0, batch index: 40200, train loss: 2.319406\n",
      "==>>> epoch: 0, batch index: 40300, train loss: 2.315337\n",
      "==>>> epoch: 0, batch index: 40400, train loss: 2.305547\n",
      "==>>> epoch: 0, batch index: 40500, train loss: 2.322787\n",
      "==>>> epoch: 0, batch index: 40600, train loss: 2.320456\n",
      "==>>> epoch: 0, batch index: 40700, train loss: 2.332757\n",
      "==>>> epoch: 0, batch index: 40800, train loss: 2.323625\n",
      "==>>> epoch: 0, batch index: 40900, train loss: 2.333398\n",
      "==>>> epoch: 0, batch index: 41000, train loss: 2.339132\n",
      "==>>> epoch: 0, batch index: 41100, train loss: 2.337515\n",
      "==>>> epoch: 0, batch index: 41200, train loss: 2.311435\n",
      "==>>> epoch: 0, batch index: 41300, train loss: 2.329234\n",
      "==>>> epoch: 0, batch index: 41400, train loss: 2.311056\n",
      "==>>> epoch: 0, batch index: 41500, train loss: 2.309727\n",
      "==>>> epoch: 0, batch index: 41600, train loss: 2.313888\n",
      "==>>> epoch: 0, batch index: 41700, train loss: 2.335157\n",
      "==>>> epoch: 0, batch index: 41800, train loss: 2.325774\n",
      "==>>> epoch: 0, batch index: 41900, train loss: 2.334593\n",
      "==>>> epoch: 0, batch index: 42000, train loss: 2.323034\n",
      "==>>> epoch: 0, batch index: 42100, train loss: 2.325837\n",
      "==>>> epoch: 0, batch index: 42200, train loss: 2.318073\n",
      "==>>> epoch: 0, batch index: 42300, train loss: 2.313127\n",
      "==>>> epoch: 0, batch index: 42400, train loss: 2.339701\n",
      "==>>> epoch: 0, batch index: 42500, train loss: 2.330647\n",
      "==>>> epoch: 0, batch index: 42600, train loss: 2.324767\n",
      "==>>> epoch: 0, batch index: 42700, train loss: 2.324346\n",
      "==>>> epoch: 0, batch index: 42800, train loss: 2.314134\n",
      "==>>> epoch: 0, batch index: 42900, train loss: 2.307680\n",
      "==>>> epoch: 0, batch index: 43000, train loss: 2.330532\n",
      "==>>> epoch: 0, batch index: 43100, train loss: 2.336263\n",
      "==>>> epoch: 0, batch index: 43200, train loss: 2.328989\n",
      "==>>> epoch: 0, batch index: 43300, train loss: 2.329215\n",
      "==>>> epoch: 0, batch index: 43400, train loss: 2.323168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 43500, train loss: 2.335743\n",
      "==>>> epoch: 0, batch index: 43600, train loss: 2.334287\n",
      "==>>> epoch: 0, batch index: 43700, train loss: 2.326617\n",
      "==>>> epoch: 0, batch index: 43800, train loss: 2.326769\n",
      "==>>> epoch: 0, batch index: 43900, train loss: 2.319739\n",
      "==>>> epoch: 0, batch index: 44000, train loss: 2.319224\n",
      "==>>> epoch: 0, batch index: 44100, train loss: 2.295642\n",
      "==>>> epoch: 0, batch index: 44200, train loss: 2.341585\n",
      "==>>> epoch: 0, batch index: 44300, train loss: 2.327471\n",
      "==>>> epoch: 0, batch index: 44400, train loss: 2.317720\n",
      "==>>> epoch: 0, batch index: 44500, train loss: 2.311874\n",
      "==>>> epoch: 0, batch index: 44600, train loss: 2.323528\n",
      "==>>> epoch: 0, batch index: 44700, train loss: 2.304475\n",
      "==>>> epoch: 0, batch index: 44800, train loss: 2.316072\n",
      "==>>> epoch: 0, batch index: 44900, train loss: 2.310189\n",
      "==>>> epoch: 0, batch index: 45000, train loss: 2.334901\n",
      "==>>> epoch: 0, batch index: 45100, train loss: 2.333777\n",
      "==>>> epoch: 0, batch index: 45200, train loss: 2.322398\n",
      "==>>> epoch: 0, batch index: 45300, train loss: 2.329449\n",
      "==>>> epoch: 0, batch index: 45400, train loss: 2.318808\n",
      "==>>> epoch: 0, batch index: 45500, train loss: 2.309400\n",
      "==>>> epoch: 0, batch index: 45600, train loss: 2.327893\n",
      "==>>> epoch: 0, batch index: 45700, train loss: 2.316673\n",
      "==>>> epoch: 0, batch index: 45800, train loss: 2.313835\n",
      "==>>> epoch: 0, batch index: 45900, train loss: 2.303478\n",
      "==>>> epoch: 0, batch index: 46000, train loss: 2.321040\n",
      "==>>> epoch: 0, batch index: 46100, train loss: 2.317976\n",
      "==>>> epoch: 0, batch index: 46200, train loss: 2.333907\n",
      "==>>> epoch: 0, batch index: 46300, train loss: 2.331886\n",
      "==>>> epoch: 0, batch index: 46400, train loss: 2.318599\n",
      "==>>> epoch: 0, batch index: 46500, train loss: 2.336965\n",
      "==>>> epoch: 0, batch index: 46600, train loss: 2.322612\n",
      "==>>> epoch: 0, batch index: 46700, train loss: 2.305576\n",
      "==>>> epoch: 0, batch index: 46800, train loss: 2.316347\n",
      "==>>> epoch: 0, batch index: 46900, train loss: 2.328824\n",
      "==>>> epoch: 0, batch index: 47000, train loss: 2.310709\n",
      "==>>> epoch: 0, batch index: 47100, train loss: 2.319404\n",
      "==>>> epoch: 0, batch index: 47200, train loss: 2.286281\n",
      "==>>> epoch: 0, batch index: 47300, train loss: 2.303631\n",
      "==>>> epoch: 0, batch index: 47400, train loss: 2.319183\n",
      "==>>> epoch: 0, batch index: 47500, train loss: 2.334492\n",
      "==>>> epoch: 0, batch index: 47600, train loss: 2.300639\n",
      "==>>> epoch: 0, batch index: 47700, train loss: 2.336618\n",
      "==>>> epoch: 0, batch index: 47800, train loss: 2.336521\n",
      "==>>> epoch: 0, batch index: 47900, train loss: 2.325166\n",
      "==>>> epoch: 0, batch index: 48000, train loss: 2.321689\n",
      "==>>> epoch: 0, batch index: 48100, train loss: 2.320861\n",
      "==>>> epoch: 0, batch index: 48200, train loss: 2.307920\n",
      "==>>> epoch: 0, batch index: 48300, train loss: 2.322902\n",
      "==>>> epoch: 0, batch index: 48400, train loss: 2.293788\n",
      "==>>> epoch: 0, batch index: 48500, train loss: 2.307258\n",
      "==>>> epoch: 0, batch index: 48600, train loss: 2.329686\n",
      "==>>> epoch: 0, batch index: 48700, train loss: 2.335076\n",
      "==>>> epoch: 0, batch index: 48800, train loss: 2.334246\n",
      "==>>> epoch: 0, batch index: 48900, train loss: 2.314834\n",
      "==>>> epoch: 0, batch index: 49000, train loss: 2.330102\n",
      "==>>> epoch: 0, batch index: 49100, train loss: 2.330573\n",
      "==>>> epoch: 0, batch index: 49200, train loss: 2.323033\n",
      "==>>> epoch: 0, batch index: 49300, train loss: 2.334581\n",
      "==>>> epoch: 0, batch index: 49400, train loss: 2.324560\n",
      "==>>> epoch: 0, batch index: 49500, train loss: 2.313134\n",
      "==>>> epoch: 0, batch index: 49600, train loss: 2.308722\n",
      "==>>> epoch: 0, batch index: 49700, train loss: 2.310266\n",
      "==>>> epoch: 0, batch index: 49800, train loss: 2.325188\n",
      "==>>> epoch: 0, batch index: 49900, train loss: 2.320232\n",
      "==>>> epoch: 0, batch index: 50000, train loss: 2.331065\n",
      "==>>> epoch: 0, batch index: 50100, train loss: 2.335561\n",
      "==>>> epoch: 0, batch index: 50200, train loss: 2.319249\n",
      "==>>> epoch: 0, batch index: 50300, train loss: 2.320616\n",
      "==>>> epoch: 0, batch index: 50400, train loss: 2.332250\n",
      "==>>> epoch: 0, batch index: 50500, train loss: 2.301207\n",
      "==>>> epoch: 0, batch index: 50600, train loss: 2.268931\n",
      "==>>> epoch: 0, batch index: 50700, train loss: 2.322991\n",
      "==>>> epoch: 0, batch index: 50800, train loss: 2.308834\n",
      "==>>> epoch: 0, batch index: 50900, train loss: 2.344398\n",
      "==>>> epoch: 0, batch index: 51000, train loss: 2.328199\n",
      "==>>> epoch: 0, batch index: 51100, train loss: 2.335291\n",
      "==>>> epoch: 0, batch index: 51200, train loss: 2.325278\n",
      "==>>> epoch: 0, batch index: 51300, train loss: 2.343747\n",
      "==>>> epoch: 0, batch index: 51400, train loss: 2.328648\n",
      "==>>> epoch: 0, batch index: 51500, train loss: 2.328888\n",
      "==>>> epoch: 0, batch index: 51600, train loss: 2.330894\n",
      "==>>> epoch: 0, batch index: 51700, train loss: 2.310672\n",
      "==>>> epoch: 0, batch index: 51800, train loss: 2.328352\n",
      "==>>> epoch: 0, batch index: 51900, train loss: 2.316664\n",
      "==>>> epoch: 0, batch index: 52000, train loss: 2.331699\n",
      "==>>> epoch: 0, batch index: 52100, train loss: 2.340532\n",
      "==>>> epoch: 0, batch index: 52200, train loss: 2.319950\n",
      "==>>> epoch: 0, batch index: 52300, train loss: 2.307508\n",
      "==>>> epoch: 0, batch index: 52400, train loss: 2.327694\n",
      "==>>> epoch: 0, batch index: 52500, train loss: 2.328574\n",
      "==>>> epoch: 0, batch index: 52600, train loss: 2.342513\n",
      "==>>> epoch: 0, batch index: 52700, train loss: 2.318664\n",
      "==>>> epoch: 0, batch index: 52800, train loss: 2.320244\n",
      "==>>> epoch: 0, batch index: 52900, train loss: 2.329234\n",
      "==>>> epoch: 0, batch index: 53000, train loss: 2.331801\n",
      "==>>> epoch: 0, batch index: 53100, train loss: 2.329103\n",
      "==>>> epoch: 0, batch index: 53200, train loss: 2.320515\n",
      "==>>> epoch: 0, batch index: 53300, train loss: 2.315931\n",
      "==>>> epoch: 0, batch index: 53400, train loss: 2.312057\n",
      "==>>> epoch: 0, batch index: 53500, train loss: 2.323083\n",
      "==>>> epoch: 0, batch index: 53600, train loss: 2.328476\n",
      "==>>> epoch: 0, batch index: 53700, train loss: 2.323365\n",
      "==>>> epoch: 0, batch index: 53800, train loss: 2.315682\n",
      "==>>> epoch: 0, batch index: 53900, train loss: 2.313596\n",
      "==>>> epoch: 0, batch index: 54000, train loss: 2.319972\n",
      "==>>> epoch: 0, batch index: 54100, train loss: 2.320122\n",
      "==>>> epoch: 0, batch index: 54200, train loss: 2.331713\n",
      "==>>> epoch: 0, batch index: 54300, train loss: 2.325735\n",
      "==>>> epoch: 0, batch index: 54400, train loss: 2.326332\n",
      "==>>> epoch: 0, batch index: 54500, train loss: 2.326671\n",
      "==>>> epoch: 0, batch index: 54600, train loss: 2.332525\n",
      "==>>> epoch: 0, batch index: 54700, train loss: 2.324234\n",
      "==>>> epoch: 0, batch index: 54800, train loss: 2.318044\n",
      "==>>> epoch: 0, batch index: 54900, train loss: 2.331834\n",
      "==>>> epoch: 0, batch index: 55000, train loss: 2.330825\n",
      "==>>> epoch: 0, batch index: 55100, train loss: 2.325599\n",
      "==>>> epoch: 0, batch index: 55200, train loss: 2.297510\n",
      "==>>> epoch: 0, batch index: 55300, train loss: 2.335365\n",
      "==>>> epoch: 0, batch index: 55400, train loss: 2.319886\n",
      "==>>> epoch: 0, batch index: 55500, train loss: 2.332445\n",
      "==>>> epoch: 0, batch index: 55600, train loss: 2.334032\n",
      "==>>> epoch: 0, batch index: 55700, train loss: 2.308598\n",
      "==>>> epoch: 0, batch index: 55800, train loss: 2.295140\n",
      "==>>> epoch: 0, batch index: 55900, train loss: 2.305332\n",
      "==>>> epoch: 0, batch index: 56000, train loss: 2.304568\n",
      "==>>> epoch: 0, batch index: 56100, train loss: 2.314712\n",
      "==>>> epoch: 0, batch index: 56200, train loss: 2.324671\n",
      "==>>> epoch: 0, batch index: 56300, train loss: 2.320727\n",
      "==>>> epoch: 0, batch index: 56400, train loss: 2.335792\n",
      "==>>> epoch: 0, batch index: 56500, train loss: 2.297291\n",
      "==>>> epoch: 0, batch index: 56600, train loss: 2.325319\n",
      "==>>> epoch: 0, batch index: 56700, train loss: 2.297121\n",
      "==>>> epoch: 0, batch index: 56800, train loss: 2.334840\n",
      "==>>> epoch: 0, batch index: 56900, train loss: 2.339068\n",
      "==>>> epoch: 0, batch index: 57000, train loss: 2.339852\n",
      "==>>> epoch: 0, batch index: 57100, train loss: 2.297060\n",
      "==>>> epoch: 0, batch index: 57200, train loss: 2.331979\n",
      "==>>> epoch: 0, batch index: 57300, train loss: 2.331471\n",
      "==>>> epoch: 0, batch index: 57400, train loss: 2.332146\n",
      "==>>> epoch: 0, batch index: 57500, train loss: 2.313310\n",
      "==>>> epoch: 0, batch index: 57600, train loss: 2.301211\n",
      "==>>> epoch: 0, batch index: 57700, train loss: 2.324683\n",
      "==>>> epoch: 0, batch index: 57800, train loss: 2.328945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, batch index: 57900, train loss: 2.298194\n",
      "==>>> epoch: 0, batch index: 58000, train loss: 2.319130\n",
      "==>>> epoch: 0, batch index: 58100, train loss: 2.337333\n",
      "==>>> epoch: 0, batch index: 58200, train loss: 2.322055\n",
      "==>>> epoch: 0, batch index: 58300, train loss: 2.331386\n",
      "==>>> epoch: 0, batch index: 58400, train loss: 2.309884\n",
      "==>>> epoch: 0, batch index: 58500, train loss: 2.312867\n",
      "==>>> epoch: 0, batch index: 58600, train loss: 2.320931\n",
      "==>>> epoch: 0, batch index: 58700, train loss: 2.315152\n",
      "==>>> epoch: 0, batch index: 58800, train loss: 2.312148\n",
      "==>>> epoch: 0, batch index: 58900, train loss: 2.329104\n",
      "==>>> epoch: 0, batch index: 59000, train loss: 2.311904\n",
      "==>>> epoch: 0, batch index: 59100, train loss: 2.337870\n",
      "==>>> epoch: 0, batch index: 59200, train loss: 2.311359\n",
      "==>>> epoch: 0, batch index: 59300, train loss: 2.298635\n",
      "==>>> epoch: 0, batch index: 59400, train loss: 2.323130\n",
      "==>>> epoch: 0, batch index: 59500, train loss: 2.330997\n",
      "==>>> epoch: 0, batch index: 59600, train loss: 2.323296\n",
      "==>>> epoch: 0, batch index: 59700, train loss: 2.340885\n",
      "==>>> epoch: 0, batch index: 59800, train loss: 2.344110\n",
      "==>>> epoch: 0, batch index: 59900, train loss: 2.319683\n",
      "==>>> epoch: 0, batch index: 60000, train loss: 2.304352\n",
      "==>>> epoch: 0, batch index: 100, test loss: 1.450764, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 200, test loss: 2.019037, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 300, test loss: 2.230891, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 400, test loss: 2.309540, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 500, test loss: 2.335222, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 600, test loss: 2.332856, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 700, test loss: 2.325751, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 800, test loss: 2.329889, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 900, test loss: 2.321917, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 1000, test loss: 2.311976, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 1100, test loss: 2.336780, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 1200, test loss: 2.311978, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 1300, test loss: 2.342883, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 1400, test loss: 2.323439, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 1500, test loss: 2.351009, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 1600, test loss: 2.311867, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 1700, test loss: 2.350181, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 1800, test loss: 2.337232, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 1900, test loss: 2.336030, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 2000, test loss: 2.320373, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 2100, test loss: 2.333426, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 2200, test loss: 2.316127, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 2300, test loss: 2.327681, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 2400, test loss: 2.302829, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 2500, test loss: 2.322571, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 2600, test loss: 2.375753, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 2700, test loss: 2.378417, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 2800, test loss: 2.368564, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 2900, test loss: 2.324331, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 3000, test loss: 2.380009, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 3100, test loss: 2.329995, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 3200, test loss: 2.317449, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 3300, test loss: 2.289599, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 3400, test loss: 2.294203, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 3500, test loss: 2.319044, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 3600, test loss: 2.329087, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 3700, test loss: 2.321186, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 3800, test loss: 2.337349, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 3900, test loss: 2.399165, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 4000, test loss: 2.375972, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 4100, test loss: 2.363904, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 4200, test loss: 2.350036, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 4300, test loss: 2.373065, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 4400, test loss: 2.404442, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 4500, test loss: 2.340819, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 4600, test loss: 2.326994, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 4700, test loss: 2.324834, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 4800, test loss: 2.322496, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 4900, test loss: 2.315251, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 5000, test loss: 2.321660, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 5100, test loss: 2.314988, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 5200, test loss: 2.325908, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 5300, test loss: 2.337350, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 5400, test loss: 2.338328, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 5500, test loss: 2.320771, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 5600, test loss: 2.338216, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 5700, test loss: 2.336960, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 5800, test loss: 2.331071, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 5900, test loss: 2.349182, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 6000, test loss: 2.354191, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 6100, test loss: 2.342944, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 6200, test loss: 2.332545, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 6300, test loss: 2.338119, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 6400, test loss: 2.321494, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 6500, test loss: 2.336440, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 6600, test loss: 2.345491, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 6700, test loss: 2.292191, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 6800, test loss: 2.315202, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 6900, test loss: 2.331440, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 7000, test loss: 2.357463, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 7100, test loss: 2.327576, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 7200, test loss: 2.324649, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 7300, test loss: 2.334919, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 7400, test loss: 2.342640, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 7500, test loss: 2.354327, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 7600, test loss: 2.349856, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 7700, test loss: 2.359830, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 7800, test loss: 2.341168, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 7900, test loss: 2.360795, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 8000, test loss: 2.337775, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 8100, test loss: 2.339693, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 8200, test loss: 2.345038, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 8300, test loss: 2.329742, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 8400, test loss: 2.310176, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 8500, test loss: 2.323856, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 8600, test loss: 2.326894, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 8700, test loss: 2.335949, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 8800, test loss: 2.341545, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 8900, test loss: 2.337459, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 9000, test loss: 2.334154, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 9100, test loss: 2.311245, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 9200, test loss: 2.332618, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 9300, test loss: 2.352820, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 9400, test loss: 2.343454, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 9500, test loss: 2.344779, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 9600, test loss: 2.334299, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 9700, test loss: 2.326999, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 9800, test loss: 2.324006, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 9900, test loss: 2.317657, acc: 0.000\n",
      "==>>> epoch: 0, batch index: 10000, test loss: 2.303057, acc: 0.000\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## training\n",
    "model = LeNet()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "for epoch in range(1):\n",
    "    # training\n",
    "    ave_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_dloader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        train_loss_history.append(loss.item())\n",
    "        ave_loss = ave_loss * 0.99 + loss.item() * 0.01\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_dloader):\n",
    "            print ('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss))\n",
    "    # testing\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    total_cnt = 0\n",
    "    for batch_idx, (x, target) in enumerate(test_dloader):\n",
    "        with torch.no_grad():\n",
    "            x, target = Variable(x), Variable(target)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        test_loss_history.append(loss.item())\n",
    "        _, pred_label = torch.max(out.data, 1)\n",
    "        total_cnt += x.data.size()[0]\n",
    "        correct_cnt += (pred_label == target.data).sum()\n",
    "        # smooth average\n",
    "        ave_loss = ave_loss * 0.99 + loss.item() * 0.01\n",
    "        \n",
    "        if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_dloader):\n",
    "            print ('==>>> epoch: {}, batch index: {}, test loss: {:.6f}, acc: {:.3f}'.format(\n",
    "                epoch, batch_idx+1, ave_loss, correct_cnt * 1.0 / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "            \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss_history, label=\"train\");\n",
    "ax.plot(test_loss_history, label=\"test\");\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
